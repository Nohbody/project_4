{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Sequential Structure from Motion\n",
    "\n",
    "### Due 4/3/2019\n",
    "\n",
    "### Graduate Students: Our next reading is [Snavely, 2006](http://195.130.87.21:8080/dspace/bitstream/123456789/636/1/Photo%20tourism%20exploring%20photo%20collections%20in%203D.pdf).  We'll have a written report on this one: these methods papers aren't as good for discussions as I'd hoped.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You have now developed code that takes two photographs, finds matching locations in both, determines the relative motion between the cameras that took both photos, and solves for the 3D position of those points using epipolar geometry.  **The next (and final for our purposes) stage is to extend this analysis to more than two images**, such that we can build 3D models of objects on the ground with just about as much detail as we'd like.\n",
    "\n",
    "## Adding the third photo\n",
    "How do we add these additional photos?  To be perfectly honest, at this point it's mostly an exercise in match housekeeping: we've already developed most of the code that we need.  First, let's consider what we've got after matching our first two images, $I_1$ and $I_2$.  First, we have a set of keypoints in each image, associated with a set of matches.  These matches have been quality controlled twice:  first by the ratio test, then by RANSAC in conjunction with the recovery of the essential matrix.  Assuming that we've used our known camera matrix to convert our pixel-wise coordinates to generalized coordinates, let's call these keypoints $\\mathbf{x}_1$ and $\\mathbf{x}_2$.  In practice, we can drop all of those keypoints for which there is not an associated accepted match.  Then, for each of our kept matches, we have the essential matrix $E_{12}$, from which we can extract a pair of projection matrices $\\mathbf{P}_1 = [\\mathbf{I}|\\mathbf{0}]$ and $\\mathbf{P}_2 = [\\mathbf{R}_2|\\mathbf{t}_2]$.  Using these projection matrices, we generated 3D, world coordinate location of the corresponding features that showed up robustly in both images.  We'll call these coordinates $\\mathbf{X}_{12}$.  \n",
    "\n",
    "To add a third image $\\mathbf{I}_3$ to the mix, consider that the situation outlined above is sort of analogous to the information that we have when we want to do pose estimation with ground control points: we have 3D world coordinates as well as the image coordinates of a set of points (a bunch of them, usually!), and we want to recover the camera pose.  The problem is that the feature generalized coordinates that we've computed are for $I_1$ and $I_2$, but not $I_3$.  Is this a big problem?  Of course not!  We can simply find $\\mathbf{x}_3$ in $I_3$ that correspond to $\\mathbf{x}_2$, the keypoints in the second image.  Then we identify these keypoints with the 3D poi nts $\\mathbf{X}_{12}$.  Thus we have image coordinates of features in the third image and the corresponding world coordinates: we can now perform pose estimation, just as we did in Project 1.  \n",
    "\n",
    "Of course there are a few minor caveats: first, we need to filter out spurious matches between $\\mathbf{x}_2$ and $\\mathbf{x}_3$.  To do this, we can utilize a tool that we already have: RANSAC estimation of the essential matrix.  Because $I_2$ and $I_3$ are related by epipolar geometry just as $I_1$ and $I_2$ are, we can use the same subroutine to compute the essential matrix $\\mathbf{E}_{23}$, and (critically) identify and filter outliers, i.e. we'll discard matches that don't don't correspond to the consensus view of the essential matrix.  This also leads to the next caveat, namely that we need an initial guess (call it $P_3^0$) for pose estimation to converge properly.  Where should this initial guess come from?  The $\\mathbf{E}_{23}$ provides a rotation given as if camera 2 were canonical, i.e. $\\mathbf{P_2'}=[\\mathbf{I}|\\mathbf{0}]$, $\\mathbf{P_3}'=[\\mathbf{R}_3'|\\mathbf{t}_3']$.  We'll call it $P_3'$.  We need to convert this projection matrix to a coordinate system in which $I_1$ (not $I_2$) is canonical.  Fortunately, this is easy:\n",
    "$$\n",
    "P_3 \\approx P_2 P_3'.\n",
    "$$\n",
    "This $P_3$, is a an excellent initial guess for pose estimation (in principle, it's rotation matrix should actually be correct).  Note that the translation component is only good up to a constant: however, this isn't too problematic because its direction is close to correct, and any optimization just needs to perform what amounts to a line search (a univariate optimization problem) to find the correct scaling. \n",
    "\n",
    "Once we have a robust estimation of the third camera's pose, we can use it do point triangulation on the correspondences between $I_2$ and $I_3$ not associated with an already-known world coordinate point, which allows us to augment our 3D model with new points.  Additionally, we can perform triangulation with *3 views*, potentially improving our accuracy.  Moreover, we can apply the process above iteratively, adding more and more images to generate a highly featured 3D model from (for example) 360 degrees worth of view angles.  \n",
    "\n",
    "## Application\n",
    "**Generate code that performs the above process for a third image.  Apply it to one of the 3D image datasets that we generated in class.  Note that we will be collecting aerial imagery from drones as well.  Apply this method to a sequence of drone imagery as well.**  As a challenge, can you implement code that sequentially adds an arbitrary number of images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import piexif\n",
    "import scipy.optimize as so\n",
    "\n",
    "def get_best_sift_matches(I_1, I_2, r=0.7):\n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n",
    "    kp1, des1 = sift.detectAndCompute(I_1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(I_2, None)\n",
    "\n",
    "    matcher = cv2.BFMatcher()\n",
    "    matches = matcher.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        # Compute the ratio between best match m, and second best match n here\n",
    "        if m.distance < r * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    u1 = []\n",
    "    u2 = []\n",
    "\n",
    "    for g in good_matches:\n",
    "        u1.append(kp1[g.queryIdx].pt)\n",
    "        u2.append(kp2[g.trainIdx].pt)\n",
    "\n",
    "    u1 = np.array(u1)\n",
    "    u2 = np.array(u2)\n",
    "\n",
    "    u1 = np.c_[u1, np.ones(u1.shape[0])]\n",
    "    u2 = np.c_[u2, np.ones(u2.shape[0])]\n",
    "\n",
    "    return u1, u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondence_points(I_1, img_name, u1, u2):\n",
    "    h,w,d = I_1.shape\n",
    "    exif = piexif.load(img_name)\n",
    "    f = exif['Exif'][piexif.ExifIFD.FocalLengthIn35mmFilm]/36*w\n",
    "    cu = w//2\n",
    "    cv = h//2\n",
    "\n",
    "    K_cam = np.array([[f,0,cu], [0,f,cv], [0,0,1]])\n",
    "    K_inv = np.linalg.inv(K_cam)\n",
    "    x1 = u1 @ K_inv.T\n",
    "    x2 = u2 @ K_inv.T \n",
    "    \n",
    "    # compute E and inliers\n",
    "    E, inliers = cv2.findEssentialMat(x1[:, :2], x2[:, :2], np.eye(3), method=cv2.RANSAC, threshold=1e-3)\n",
    "    inliers = inliers.ravel().astype(bool)\n",
    "    \n",
    "    # recover pose\n",
    "    n_in, R, t,_ = cv2.recoverPose(E, x1[inliers,:2], x2[inliers,:2])\n",
    "\n",
    "    # compute camera matrices\n",
    "    P_1 = np.array([[1,0,0,0],\n",
    "                [0,1,0,0],\n",
    "                [0,0,1,0]])\n",
    "    P_2 = np.hstack((R, t))\n",
    "    \n",
    "    # convert image to camera coordinates\n",
    "    P_1c = K_cam @ P_1\n",
    "    P_2c = K_cam @ P_2\n",
    "        \n",
    "    # triangulate using linear solution for each point in x1, and x2\n",
    "    point_cloud = []\n",
    "    for a,b in zip(x1[inliers,:], x2[inliers,:]):\n",
    "        t_point = triangulate(P_1c, P_2c, a, b)\n",
    "        t_point /= t_point[-1] \n",
    "        point_cloud.append(t_point[:3])\n",
    "        \n",
    "    return point_cloud, P_1, P_2, P_1c, P_2c, x1[inliers, :], x2[inliers, :], inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(P0,P1,x1,x2):\n",
    "    # P0,P1: projection matrices for each of two cameras/images\n",
    "    # x1,x1: corresponding points in each of two images (If using P that has been scaled by K, then use camera\n",
    "    # coordinates, otherwise use generalized coordinates)\n",
    "    A = np.array([[P0[2,0]*x1[0] - P0[0,0], P0[2,1]*x1[0] - P0[0,1], P0[2,2]*x1[0] - P0[0,2], P0[2,3]*x1[0] - P0[0,3]],\n",
    "                  [P0[2,0]*x1[1] - P0[1,0], P0[2,1]*x1[1] - P0[1,1], P0[2,2]*x1[1] - P0[1,2], P0[2,3]*x1[1] - P0[1,3]],\n",
    "                  [P1[2,0]*x2[0] - P1[0,0], P1[2,1]*x2[0] - P1[0,1], P1[2,2]*x2[0] - P1[0,2], P1[2,3]*x2[0] - P1[0,3]],\n",
    "                  [P1[2,0]*x2[1] - P1[1,0], P1[2,1]*x2[1] - P1[1,1], P1[2,2]*x2[1] - P1[1,2], P1[2,3]*x2[1] - P1[1,3]]])\n",
    "    u,s,vt = np.linalg.svd(A)\n",
    "    return vt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera(object):\n",
    "    def __init__(self):\n",
    "        self.p = None                   # Pose\n",
    "        self.f = None                   # Focal Length in Pixels\n",
    "        self.c = np.array([None,None])  # Sensor\n",
    "        \n",
    "    def projective_transform(self,x):\n",
    "        \"\"\"  \n",
    "        This function performs the projective transform on generalized coordinates in the camera reference frame.\n",
    "        \"\"\"\n",
    "        focal = self.f\n",
    "        sensor = self.c\n",
    "        \n",
    "        #General Coordinates\n",
    "        gcx = x[0]/x[2]\n",
    "        gcy = x[1]/x[2]\n",
    "        \n",
    "        #Pixel Locations\n",
    "        pu = gcx*focal + sensor[0]/2.\n",
    "        pv = gcy*focal + sensor[1]/2.\n",
    "        \n",
    "        return np.array([pu,pv])\n",
    "      \n",
    "    \n",
    "    def rotational_transform(self,X,pts):\n",
    "        \"\"\"  \n",
    "        This function performs the translation and rotation from world coordinates into generalized camera coordinates.\n",
    "        \"\"\"\n",
    "        \n",
    "        cam_x = X[0]\n",
    "        cam_y = X[1]\n",
    "        cam_z = X[2]\n",
    "        roll = X[3]\n",
    "        pitch = X[4]\n",
    "        yaw = X[5]\n",
    "        \n",
    "        r_axis = np.array([[1, 0, 0], [0, 0,-1], [0, 1, 0]])\n",
    "        r_roll = np.array([[np.cos(roll), 0, -1*np.sin(roll)], [0, 1, 0], [np.sin(roll), 0, np.cos(roll)]])\n",
    "        r_pitch = np.array([[1, 0, 0], [0, np.cos(pitch), np.sin(pitch)], [0, -1*np.sin(pitch), np.cos(pitch)]])\n",
    "        r_yaw = np.array([[np.cos(yaw), -1*np.sin(yaw), 0, 0], [np.sin(yaw), np.cos(yaw), 0, 0], [0, 0, 1, 0]])\n",
    "\n",
    "        T = np.array([[1, 0, 0, -cam_x],[0, 1, 0, -cam_y], [0, 0, 1, -cam_z], [0, 0, 0, 1]])\n",
    "        \n",
    "        C = r_axis @ r_roll @ r_pitch @ r_yaw @ T\n",
    "\n",
    "        return C @ pts\n",
    "    \n",
    "    def rotational_transform_x(self, X):\n",
    "        \"\"\"\n",
    "        This function performs the translation and rotation from world coordinates into generalized camera coordinates.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack pose? could do something different here.\n",
    "        X_cam, Y_cam, Z_cam, azimuth_cam_deg, pitch_cam_deg, roll_cam_deg = self.p\n",
    "\n",
    "        # Make X a set of homogeneous coors\n",
    "        X = np.vstack((X, np.ones(X.shape[1])))\n",
    "\n",
    "        # Convert degrees to radians\n",
    "        azimuth_cam_rad = np.deg2rad(azimuth_cam_deg)\n",
    "        pitch_cam_rad = np.deg2rad(pitch_cam_deg)\n",
    "        roll_cam_rad = np.deg2rad(roll_cam_deg)\n",
    "\n",
    "        translation_vec = [X_cam, Y_cam, Z_cam]\n",
    "        C = self.make_cam_mtx(azimuth_cam_rad, pitch_cam_rad, roll_cam_rad, translation_vec)\n",
    "\n",
    "        return C @ X\n",
    "    \n",
    "    def estimate_pose(self,gcps):\n",
    "        \"\"\"\n",
    "        This function adjusts the pose vector such that the difference between the observed pixel coordinates u_gcp \n",
    "        and the projected pixels coordinates of X_gcp is minimized.\n",
    "        \"\"\"\n",
    "        p_opts = [] # Initial easting, northing, elevation positions\n",
    "        u_gcps = [] # Initial projected u,v coordinates\n",
    "\n",
    "        for gcp in gcps:\n",
    "            pts = gcp[2:5]\n",
    "            # Add homogenous coordinate\n",
    "            pts= np.append(pts, 1)\n",
    "\n",
    "            # Observed pixel coordinates\n",
    "            u_gcp = gcp[0:2]\n",
    "            p_opts.append(pts)\n",
    "            u_gcps.append(u_gcp)\n",
    "    \n",
    "        ave_easting = np.mean([gcp[2] for gcp in gcps])\n",
    "        ave_northing = np.mean([gcp[3] for gcp in gcps])\n",
    "        ave_elevation = np.mean([gcp[4] for gcp in gcps])\n",
    "            \n",
    "        # Initial guess at the pose\n",
    "        p0 = np.array([ave_easting, ave_northing, ave_elevation,90,45,45])\n",
    "        \n",
    "        # print(p0, '\\n', p_opts, '\\n', u_gcps, '\\n')\n",
    "        \n",
    "        # Find the optimal pose minimizing the residual\n",
    "        p_opt = so.least_squares(self.residual, p0, method='lm', args=(p_opts,u_gcps))['x']\n",
    "        \n",
    "        self.p = p_opt\n",
    "        \n",
    "    # Function to transform all gcps into the camera coordinates\n",
    "    def transform_all(self, X_gcp, p):\n",
    "        transformed = []\n",
    "        \n",
    "        for gcp in X_gcp:\n",
    "            # Perform rotational transform on X_gcp\n",
    "            rotated = self.rotational_transform(p, gcp)\n",
    "            \n",
    "            # Project the rotated coordinates to pixel coordinates\n",
    "            projected = self.projective_transform(rotated)\n",
    "            transformed.append(projected)\n",
    "\n",
    "        return transformed\n",
    "    \n",
    "    # Calculate the difference in the estimate projection of X_gcp and actual positions of u_gcp\n",
    "    def residual(self, p0, X_gcp, u_gcp):\n",
    "        all_res = np.array(self.transform_all(X_gcp, p0)) - np.array(u_gcp)\n",
    "        all_res = all_res.ravel()\n",
    "        \n",
    "        return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48485876 -0.73058377  0.48079032 -1.67283012]\n",
      " [ 0.73282444  0.63942302  0.23260811 -0.85833798]\n",
      " [-0.47736811  0.23955282  0.8454195   0.56530148]]\n"
     ]
    }
   ],
   "source": [
    "img1 = plt.imread(\"./pictures/cube/DSC03896.JPG\")\n",
    "img2 = plt.imread(\"./pictures/cube/DSC03897.JPG\")\n",
    "img3 = plt.imread(\"./pictures/cube/DSC03898.JPG\")\n",
    "\n",
    "u11, u21 = get_best_sift_matches(img1, img2, r=0.7)\n",
    "X12, P_1, P_2, P_1c, P_2c, x1, x2, in12 = get_correspondence_points(img1, \"./pictures/cube/DSC03896.JPG\", u11, u21)\n",
    "    \n",
    "u22, u31 = get_best_sift_matches(img2, img3, r=0.7)\n",
    "X23, P_2p, P_3p, P_2pc, P_3pc, x2_p, x3, in23 = get_correspondence_points(img2, \"./pictures/cube/DSC03897.JPG\", u22, u31)\n",
    "\n",
    "P_2 = np.vstack((P_2, [0, 0, 0, 1]))\n",
    "P_3p = np.vstack((P_3p, [0, 0, 0, 1]))\n",
    "\n",
    "P_3 = P_2 @ P_3p\n",
    "\n",
    "P_3 = P_3[:3, :]\n",
    "\n",
    "print(P_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 462 605 605 462 605\n",
      "\n",
      " [[-0.28042791 -0.06006592 -0.95351062 -0.43556498  2.94391192]\n",
      " [-0.28022785 -0.05987755 -0.95351024 -0.43556488  2.94391153]\n",
      " [-0.28022785 -0.05987755 -0.95351024 -0.43556488  2.94391153]\n",
      " ...\n",
      " [ 0.28712245  0.16470879 -0.95333654 -0.43557328  2.94400224]\n",
      " [ 0.31555187  0.12221343 -0.95334143 -0.43559039  2.94401676]\n",
      " [ 0.31805132  0.12342592 -0.95334085 -0.4355904   2.94401746]] \n",
      "\n",
      " Length: 605\n"
     ]
    }
   ],
   "source": [
    "print(len(x1), len(x2), len(x2_p), len(x3), len(X12), len(X23))\n",
    "\n",
    "uv_3 = x3\n",
    "ene_3 = X23\n",
    "\n",
    "uv_3 = np.array(uv_3)[:, :2]\n",
    "ene_3 = np.array(ene_3)\n",
    "\n",
    "gcps_3 = np.hstack((uv_3, ene_3))\n",
    "\n",
    "print('\\n', gcps_3, '\\n\\n', 'Length:', len(gcps_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set of keypoints across all three images\n",
    "# # from generalized coordinates x1:x2 and x2:x3, find shared matches\n",
    "# h,w,d = img1.shape\n",
    "# exif = piexif.load(\"./pictures/cube/DSC03896.JPG\")\n",
    "# f = exif['Exif'][piexif.ExifIFD.FocalLengthIn35mmFilm]/36*w\n",
    "# cu = w//2\n",
    "# cv = h//2\n",
    "\n",
    "# K_cam = np.array([[f,0,cu], [0,f,cv], [0,0,1]])\n",
    "# K_inv = np.linalg.inv(K_cam)\n",
    "# x1 = u11 @ K_inv.T\n",
    "# x21 = u21 @ K_inv.T\n",
    "\n",
    "# h,w,d = img2.shape\n",
    "# exif = piexif.load(\"./pictures/cube/DSC03897.JPG\")\n",
    "# f = exif['Exif'][piexif.ExifIFD.FocalLengthIn35mmFilm]/36*w\n",
    "# cu = w//2\n",
    "# cv = h//2\n",
    "\n",
    "# K_cam = np.array([[f,0,cu], [0,f,cv], [0,0,1]])\n",
    "# K_inv = np.linalg.inv(K_cam)\n",
    "# x22 = u22 @ K_inv.T\n",
    "# x3 = u31 @ K_inv.T            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easting:  -0.9534031528468377\n",
      "Northing:  -0.4355359559494288\n",
      "Elevation:  2.9439100243327325\n",
      "Roll:  90.00027590400317\n",
      "Pitch:  44.995066824243345\n",
      "Yaw:  44.986708582422075\n"
     ]
    }
   ],
   "source": [
    "cam3 = Camera()\n",
    "\n",
    "h,w,d = img3.shape\n",
    "cu = w//2\n",
    "cv = h//2\n",
    "\n",
    "exif = piexif.load(\"./pictures/cube/DSC03898.JPG\")\n",
    "f = exif['Exif'][piexif.ExifIFD.FocalLengthIn35mmFilm]/36*w\n",
    "cam3.f = f\n",
    "cam3.c = np.array([4032.,3024.]) # Sensor size\n",
    "\n",
    "K_cam = np.array([[f,0,cu], [0,f,cv], [0,0,1]])\n",
    "K_inv = np.linalg.inv(K_cam)\n",
    "\n",
    "P_3c = K_cam @ P_3\n",
    "\n",
    "cam3.estimate_pose(gcps_3)\n",
    "\n",
    "print(\"Easting: \", cam3.p[0])\n",
    "print(\"Northing: \", cam3.p[1])\n",
    "print(\"Elevation: \", cam3.p[2])\n",
    "print(\"Roll: \", cam3.p[3])\n",
    "print(\"Pitch: \", cam3.p[4])\n",
    "print(\"Yaw: \", cam3.p[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False False  True  True  True\n",
      "  True False  True  True  True False False False  True  True  True False\n",
      " False False  True False  True  True False  True  True  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True  True False\n",
      " False  True  True  True  True False False  True  True False  True  True\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True False  True False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True False False False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True False\n",
      "  True False  True  True  True  True  True  True False False False  True\n",
      "  True  True False  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True False  True  True False  True\n",
      "  True False  True  True False False False False  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True] \n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True False  True  True\n",
      "  True  True False  True  True  True False  True  True  True False  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False]\n"
     ]
    }
   ],
   "source": [
    "print(in12, '\\n', in23)\n",
    "\n",
    "# Find the points in 3 that do not correspond\n",
    "# with a match in 1 and 2 to add to the point_cloud\n",
    "\n",
    "point_cloud = X12\n",
    "\n",
    "\n",
    "# # For every inlier point in 3 from the 2 and 3 match\n",
    "# for a in range(len(x3)):\n",
    "#     in_match12 = False\n",
    "    \n",
    "#     # Check for the existence of the correspondence\n",
    "#     # point in 1 and 2's match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
